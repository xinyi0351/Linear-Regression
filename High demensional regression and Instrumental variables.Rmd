---
title: "High demensional regression and Instrumental Variables"
output: html_notebook
---

**This is the 7th homework of GR5205 Linear Regression. The purpose is meant to give some practice with WLS (Weighted Linear Squares) and play around with the high dimensional estimation problems. **

# Weighted Linear Square
### Generating Data

```{r creating data}
set.seed(20)
n<-1000
error<-matrix(rnorm(n,0,sqrt(20)),n,1)
beta<-matrix(c(0,1.2),2,1)
pr<-c(1/sqrt(c(1:20)))
X<-matrix(c(rep(1,n),rbinom(n,100,.5)),nrow=n,ncol=2)
Y<-X%*%beta+error
X_bar<-matrix(NA,nrow=20,ncol=1)
Y_bar<-matrix(NA,nrow=20,ncol=1)
#index<-rmultinom(1,1000,prob=pr)
group<-matrix(NA,nrow=n,ncol=1)
for(i in 1:n){
  group[i]<-sample(1:20,1,replace=T,prob=pr)
}
for(i in 1:20){
  X_bar[i,]<-mean(X[,2][which(group==i)])
}
X_bar<-cbind(1,X_bar)
for(i in 1:20){
  Y_bar[i,]<-mean(Y[which(group==i)])
}
```

### Exploration

```{r}
# test the correlation between aggregated data and individual data
cor(X_bar[,2],Y_bar)
cor(X[,2],Y)
```

It turns out that in this case, individual level data has higher correlation. 

In normal assumption of linear regression, we have $Y = X\beta+\epsilon$. In aggregated data $\bar{Y} = \bar{X}\beta+\gamma$, the $\gamma$ is not only depends on $\epsilon$ but also depend on the given X. 

$$
\begin{aligned}
\gamma &= E(\epsilon|X_i)\\
E (\gamma|\bar{X} &= 0)\\
\\
where\; k\neq m\; we\;have\; \gamma|X&∼N(\mu,\sigma^2/n_k )\\
var(\gamma|X)&=\sigma^2/n_k \\
cov(\gamma_k,\gamma_m |X)&=0(k≠m)

\end{aligned}
$$
### Estimating parameters
##### Aggretaed data

Using OLS on the aggregated data will produce unbiased estimators for $\beta$, but the result would not aligned with the ones estimated from the individual level data. The process below is an example of the scenario. 

```{r aggregated data}
n<-20
#OLS
xtx_inv_ols<-solve(t(X_bar)%*%X_bar)
beta_ols<-xtx_inv_ols%*%t(X_bar)%*%Y_bar
Y_bar_ols<-X_bar%*%beta_ols
error_ols<-Y_bar-Y_bar_ols
sigma2_ols<-crossprod(Y_bar-X_bar%*%beta_ols)/(n-2)
var_beta_ols<-as.numeric(sigma2_ols)*xtx_inv_ols
#bounds for beta
alpha=0.05
ts<-sapply(c(alpha/2,1-alpha/2),qt,df=n-2)
bounds_ols<-sapply(ts,function(ts){
  ts*sqrt(diag(var_beta_ols))
})
matrix(rep(beta_ols,2),ncol=2)+bounds_ols
#bounds for aggregate data
upper_Y<-Y_bar_ols+qt(0.975,df=n-2)*sqrt(diag(X_bar%*%var_beta_ols%*%t(X_bar)))
lower_Y<-Y_bar_ols+qt(0.025,df=n-2)*sqrt(diag(X_bar%*%var_beta_ols%*%t(X_bar)))
#plot
plot(X_bar[,2],Y_bar,xlab="aggregate x",ylab="aggregate y")
upper_con<-smooth.spline(X_bar[,2],upper_Y,spar = 0.35)
lower_con<-smooth.spline(X_bar[,2],lower_Y,spar = 0.35)
lines(upper_con,col="blue")
lines(lower_con,col="blue")

#WLS
w<-diag(c(table(group)))
xtx_inv_wls<-solve(t(X_bar)%*%w%*%X_bar)
H<-X_bar%*%xtx_inv_wls%*%t(X_bar)%*%w
YY_hat<-(diag(n)-H)%*%Y_bar
sigma2_wls<-crossprod(sqrt(w)%*%YY_hat)/(n-2)
#W<-(1/as.numeric(sigma2_wls))*w
#xtx_inv_Wls<-solve(t(X_bar)%*%W%*%X_bar)
beta_wls<-xtx_inv_wls%*%t(X_bar)%*%w%*%Y_bar
Y_bar_wls<-X_bar%*%beta_wls
var_beta_wls<-as.numeric(sigma2_wls)*xtx_inv_wls
#bounds for beta
bounds_wls<-sapply(ts,function(ts){
  ts*sqrt(diag(var_beta_wls))
})
matrix(rep(beta_wls,2),ncol=2)+bounds_wls
#bounds for aggregate data
upper_Y_wls<-Y_bar_wls+qt(0.975,df=20-2)*sqrt(diag(X_bar%*%var_beta_wls%*%t(X_bar)))
lower_Y_wls<-Y_bar_wls+qt(0.025,df=20-2)*sqrt(diag(X_bar%*%var_beta_wls%*%t(X_bar)))
#plot
upper_con_wls<-smooth.spline(X_bar[,2],upper_Y_wls,spar=0.35)
lower_con_wls<-smooth.spline(X_bar[,2],lower_Y_wls,spar=0.35)
lines(upper_con_wls,col="red")
lines(lower_con_wls,col="red")
```

Suppose we only have access to the aggregated level of data. In the picture above, blue lines represent the results from OLS whereas red lines are the results estimated from WLS. I would perfer the latter one. In this case, the sample size in each group is different. Thus they should not be treated evenly and OLS can not be used in this case.

```{r beta}
#the confidence intervel of beta
xtx_inv<-solve(t(X)%*%X)
beta_hat<-xtx_inv%*%t(X)%*%Y
sigma2<-crossprod(Y-X%*%beta_hat)/(1000-2)
var_beta<-as.numeric(sigma2)*xtx_inv
bounds<-sapply(ts,function(ts){
  ts*sqrt(diag(var_beta))
})
matrix(rep(beta_hat,2),ncol=2)+bounds
```

##### Individual level data

To test the result from an individual level of data, start with generating data in a different way.
```{r data generating}
set.seed(100)
n<-1000
error<-matrix(rnorm(n,0,sqrt(20)),n,1)
beta<-matrix(c(0,1.2),2,1)
#set groups
group_new<-matrix(NA,nrow=n,ncol=1)
X_new<-matrix(NA,nrow=n,ncol=1)
X_new_bar<-matrix(NA,nrow=20,ncol=1)
Y_new_bar<-matrix(NA,nrow=20,ncol=1)
pr<-c(1/sqrt(c(1:20)))
for(i in 1:n){
  group_new[i]<-sample(1:20,1,replace=T,prob=pr)
}
#X_new<-cbind(X_new,index=group_new)
pr<-(group_new-10)/200+0.5
for(i in 1:n){
  X_new[i]<-rbinom(1,100,pr[i])
}
X_new<-cbind(rep(1,n),X_new)
Y_new<-X_new%*%beta+error
#compute the average
for(i in 1:20){
  X_new_bar[i,]<-mean(X_new[,2][which(group_new==i)])
}
X_new_bar<-cbind(1,X_new_bar)
for(i in 1:20){
  Y_new_bar[i,]<-mean(Y_new[which(group_new==i)])
}
```

```{r individual level}
#OLS at individual level
xtx_inv_new<-solve(t(X_new)%*%X_new)
beta_new_hat<-xtx_inv_new%*%t(X_new)%*%Y_new
Y_new_hat<-X_new%*%beta_new_hat
sigma2_new<-crossprod(Y_new-Y_new_hat)/(1000-2)
var_beta_new<-as.numeric(sigma2_new)*xtx_inv_new
#bounds for beta
bounds_new<-sapply(ts,function(ts){
  ts*sqrt(diag(var_beta_new))
})
matrix(rep(beta_new_hat,2),ncol=2)+bounds_new

#wls at aggregate data
w_new<-diag(c(table(group_new)))
xtx_inv_wls_new<-solve(t(X_new_bar)%*%w_new%*%X_new_bar)
H_new<-X_new_bar%*%xtx_inv_wls_new%*%t(X_new_bar)%*%w_new
YY_hat_new<-(diag(20)-H_new)%*%Y_new_bar
sigma2_wls_new<-crossprod(sqrt(w_new)%*%YY_hat_new)/(20-2)

beta_wls_new<-xtx_inv_wls_new%*%t(X_new_bar)%*%w_new%*%Y_new_bar
var_beta_wls_new<-as.numeric(sigma2_wls_new)*xtx_inv_wls_new
#bounds for beta
bounds_wls2<-sapply(ts,function(ts){
  ts*sqrt(diag(var_beta_wls_new))
})
matrix(rep(beta_wls_new,2),ncol=2)+bounds_wls2
```

```{r prediction}
# the point-wise 95% prediction interval for new Y values
n<-1000
x<-matrix(c(rep(1,101),0:100),nrow=101,ncol=2)
y<-x%*%beta_new_hat
var_pw<-as.numeric(sigma2_new)*(diag(101)+x%*%xtx_inv_new%*%t(x))
upper_Y_hy<-y+qt(0.975,df=n-2)*sqrt(diag(var_pw))
lower_Y_hy<-y+qt(0.025,df=n-2)*sqrt(diag(var_pw))
```

```{r visualization}
#use the beta for wls, use the sigma2 from wls
n<-20
y_bar<-x%*%beta_wls_new
var_pw2<-as.numeric(sigma2_wls_new)*(diag(101)-x%*%xtx_inv_wls_new%*%t(x))
upper_y_wls2<-y_bar+qt(0.975,df=n-2)*sqrt(diag(var_pw2))
lower_y_wls2<-y_bar+qt(0.025,df=n-2)*sqrt(diag(var_pw2))
plot(X_new[,2],Y_new,xlab="hypothetical x",ylab="hypothetical y")
lines(y,col="red")
lines(x[,2],upper_Y_hy,col="blue")
lines(x[,2],lower_Y_hy,col="blue")
lines(x[,2],upper_y_wls2,col="green")
lines(x[,2],lower_y_wls2,col="green")
```
**To conclude, scenarios to choose individual level of data or aggregated data depends on the target problems. Also it depends on the availability of the data. For example, if we trying to figure out whether students are more likely to get higher grades in morning classes, the data we collected should be average grades of classes in the mornings and other times of the day. However, if we are more curious of the performance of each student, we care more about the individual level of data and aggregated data would have no sense in this scenario.**

# Non-James-Stein's estimator

In 3 dimensions or higher, if we shrink the naive average to 0 'intelligently'. we can achieve a lower mean squared error than the naive average. Following simulation was inspired but is not a James-Stein Estimator. We defined MSE in estimating high dimension vectors, $\beta$, using an estimate $\hat{\beta}$, as $E(||\beta-\hat{\beta}||^2)$. And the theoretical MSE should be 1. MSE would be greater than zero for sure. Unless the arbitrary β has a large number and the sample mean is zero.

Simulate the data, and report the smallest $\gamma$ before the MSE starts to increase again. The smallest value for gamma is 0. When raw data shrinks to zero, the mse reach the lowest point of zero.
```{r non js}
n<-1000
p<-100
error<-rnorm(n,0,sqrt(10))
X<-matrix(NA,nrow=n,ncol=p-1)
for(i in 1:n){
  X[i,]<-runif(p-1,0,1)
}
X<-cbind(rep(1,n),X)
beta<-matrix(0,ncol=1,nrow=p)
Y<-X%*%beta+error
beta_hat_ols<-solve(t(X)%*%X)%*%t(X)%*%Y
mse<-function(gamma){
  sum((gamma*beta_hat_ols-beta)^2)
}
optimize(mse,c(-10,10))
```

```{r shrink}
mse_ols<-matrix(NA,nrow=100)
mse_shrink<-matrix(NA,nrow=100)
for(i in 1:100){
  X<-matrix(c(rep(1,n),runif(99*n,0,1)),nrow=n,ncol=p)
  beta<-matrix(0,ncol=1,nrow=p)
  Y<-X%*%beta+rnorm(n,0,sqrt(10))
  beta_sm<-solve(t(X)%*%X)%*%t(X)%*%Y
  mse_ols[i,]<-sum((beta_sm-beta)^2)
  mse_shrink[i,]<-sum((0.99*(beta_sm-2)-beta+2)^2)
}
mean(mse_ols)
mean(mse_shrink)
```

We than shrunk the vector Z to the origin. And the numerically approximate the MSE over 100 simulations for the shrinked estimator and the OLS estimator. There are no big difference between the two estimators but the shrink estimator is always smaller. I perfer shrink estimator.